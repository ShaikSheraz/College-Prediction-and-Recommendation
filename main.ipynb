{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cbd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP loaded: True TS loaded: True JEE loaded: True\n",
      "AP shape: (3078, 31)\n",
      "TS shape: (2986, 30)\n",
      "JEE shape: (64958, 14)\n",
      "Cutoffs table shape: (180174, 5)\n",
      "\n",
      "--- Dataset analysis summary ---\n",
      "Total cutoff entries: 180174\n",
      "Unique exams: 3 -> ['AP' 'TS' 'JEE']\n",
      "Unique colleges: 946\n",
      "Unique branches: 218\n",
      "Categories sample: ['ESTD' 'OC_BOYS' 'OC_GIRLS' 'SC_BOYS' 'SC_GIRLS' 'ST_BOYS' 'ST_GIRLS'\n",
      " 'BCA_BOYS' 'BCA_GIRLS' 'BCB_BOYS']\n",
      "\n",
      "Cutoff rank descriptive stats:\n",
      "count    1.801740e+05\n",
      "mean     6.646587e+04\n",
      "std      6.204989e+04\n",
      "min      0.000000e+00\n",
      "25%      4.081000e+03\n",
      "50%      5.600900e+04\n",
      "75%      1.198945e+05\n",
      "max      1.144790e+06\n",
      "Name: cutoff, dtype: float64\n",
      "Synthetic students shape: (900865, 7)\n",
      "\n",
      "Training and cross-validating: LogisticRegression\n",
      "CV Accuracy mean: 0.5895 ± 0.0046\n",
      "CV ROC-AUC mean: 0.6156 ± 0.0040\n",
      "Test Acc: 0.5924, Prec: 0.5824, Rec: 0.6816, F1: 0.6281, ROC: 0.6183\n",
      "\n",
      "Training and cross-validating: RandomForest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy mean: 0.6766 ± 0.0035\n",
      "CV ROC-AUC mean: 0.7515 ± 0.0021\n",
      "Test Acc: 0.6974, Prec: 0.6997, Rec: 0.7019, F1: 0.7008, ROC: 0.7847\n",
      "\n",
      "Training and cross-validating: MLP\n",
      "CV Accuracy mean: 0.6077 ± 0.0209\n",
      "CV ROC-AUC mean: 0.6533 ± 0.0152\n",
      "Test Acc: 0.6464, Prec: 0.6327, Rec: 0.7147, F1: 0.6712, ROC: 0.7072\n",
      "\n",
      "Training and cross-validating: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy mean: 0.6979 ± 0.0023\n",
      "CV ROC-AUC mean: 0.7761 ± 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:09:31] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.7093, Prec: 0.6897, Rec: 0.7715, F1: 0.7283, ROC: 0.7888\n",
      "\n",
      "Training regression models (probability estimation):\n",
      "Model: Huber\n",
      "CV MSE mean (sample): 0.040114879809406125\n",
      "Test MSE: 0.043959, R2: -0.0813\n",
      "Model comparison saved to outputs/model_comparison_results.csv\n",
      "\n",
      "Best model by test ROC/R2: XGBoost score: 0.788782283338432\n",
      "\n",
      "Next steps:\n",
      "- Inspect outputs/model_comparison_results.csv to see model metrics side-by-side.\n",
      "- If XGBoost was installed, it is included in the comparison and often performs best on tabular data.\n",
      "- IMPORTANT: current training data is synthetic (sampled around cutoffs) → get real student admission outcomes to validate models properly.\n",
      "- After selecting best model, integrate into your web app (Streamlit/Flask) with encoders saved (joblib).\n",
      "- Optionally run feature importance (for tree models) to discover which features matter most.\n"
     ]
    }
   ],
   "source": [
    "# AP EAMCET, TS EAMCET, JEE - Unified EDA, Visualization, and Model Comparison Notebook\n",
    "# Description: Handles three separate datasets (AP, TS, JEE). Performs cleaning, EDA,\n",
    "# visualizations, and compares multiple ML models (classification + regression) to\n",
    "# find the best predictor of admission using ranks and cutoffs.\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 1: Imports and Config\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Try to import xgboost (install if missing)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception:\n",
    "    print('xgboost not found. Attempting to install xgboost...')\n",
    "    try:\n",
    "        os.system(f'{sys.executable} -m pip install xgboost --quiet')\n",
    "        import xgboost as xgb\n",
    "        from xgboost import XGBClassifier\n",
    "        print('xgboost installed.')\n",
    "    except Exception as e:\n",
    "        print('Could not install xgboost:', e)\n",
    "        XGBClassifier = None\n",
    "\n",
    "# Notebook display settings\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "\n",
    "# File paths (update if needed)\n",
    "AP_FILE = \"/home/me3-lab/Project/data/AP_EAMCET_Cleaned_Merged.csv\"\n",
    "TS_FILE = \"/home/me3-lab/Project/data/TSEAMCET_2021_2022_2023_merged_clean.csv\"\n",
    "JEE_FILE = \"/home/me3-lab/Project/data/JEE_data.csv\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 2: Load Data\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "ap_df = load_csv(AP_FILE)\n",
    "ts_df = load_csv(TS_FILE)\n",
    "jee_df = load_csv(JEE_FILE)\n",
    "\n",
    "print('AP loaded:', ap_df is not None, 'TS loaded:', ts_df is not None, 'JEE loaded:', jee_df is not None)\n",
    "\n",
    "# Quick shape\n",
    "if ap_df is not None:\n",
    "    print('AP shape:', ap_df.shape)\n",
    "if ts_df is not None:\n",
    "    print('TS shape:', ts_df.shape)\n",
    "if jee_df is not None:\n",
    "    print('JEE shape:', jee_df.shape)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 3: Build unified cutoff table (one row per college-branch-category)\n",
    "# -----------------------------------------------------------------------------\n",
    "records = []\n",
    "\n",
    "if ap_df is not None:\n",
    "    cat_cols = [c for c in ap_df.columns if any(k in c.upper() for k in ['BOYS','GIRLS','OC','BC','SC','ST','EWS'])]\n",
    "    for _, row in ap_df.iterrows():\n",
    "        for c in cat_cols:\n",
    "            try:\n",
    "                cutoff = float(row[c])\n",
    "            except Exception:\n",
    "                continue\n",
    "            records.append({'exam':'AP','college':row.get('NAME_OF_THE_INSTITUTION',row.get('NAME',None)),'branch':row.get('branch_code',row.get('branch',None)),'category':c,'cutoff':cutoff})\n",
    "\n",
    "if ts_df is not None:\n",
    "    cat_cols = [c for c in ts_df.columns if any(k in c.lower() for k in ['boys','girls','oc','bc','sc','st','ews'])]\n",
    "    for _, row in ts_df.iterrows():\n",
    "        for c in cat_cols:\n",
    "            try:\n",
    "                cutoff = float(row[c])\n",
    "            except Exception:\n",
    "                continue\n",
    "            records.append({'exam':'TS','college':row.get('institute_name',row.get('NAME',None)),'branch':row.get('branch',None),'category':c,'cutoff':cutoff})\n",
    "\n",
    "if jee_df is not None:\n",
    "    if 'closing_rank' in jee_df.columns:\n",
    "        for _, row in jee_df.iterrows():\n",
    "            try:\n",
    "                cutoff = float(row['closing_rank'])\n",
    "            except Exception:\n",
    "                continue\n",
    "            records.append({'exam':'JEE','college':row.get('institute_short',row.get('institute',None)),'branch':row.get('program_name',row.get('branch',None)),'category':row.get('category',None),'cutoff':cutoff})\n",
    "\n",
    "cutoffs_df = pd.DataFrame(records)\n",
    "print('Cutoffs table shape:', cutoffs_df.shape)\n",
    "cutoffs_df.head()\n",
    "\n",
    "cutoffs_df.to_csv(os.path.join(OUTPUT_DIR,'cutoffs_unified.csv'), index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 4: Dataset analysis summary\n",
    "# -----------------------------------------------------------------------------\n",
    "print('\\n--- Dataset analysis summary ---')\n",
    "print('Total cutoff entries:', len(cutoffs_df))\n",
    "print('Unique exams:', cutoffs_df['exam'].nunique(), '->', cutoffs_df['exam'].unique())\n",
    "print('Unique colleges:', cutoffs_df['college'].nunique())\n",
    "print('Unique branches:', cutoffs_df['branch'].nunique())\n",
    "print('Categories sample:', cutoffs_df['category'].dropna().unique()[:10])\n",
    "print('\\nCutoff rank descriptive stats:')\n",
    "print(cutoffs_df['cutoff'].describe())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 5: Create dataset for ML (synthetic student sampling)\n",
    "# -----------------------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "student_rows = []\n",
    "for _, row in cutoffs_df.iterrows():\n",
    "    cutoff = row['cutoff']\n",
    "    if cutoff is None or cutoff <= 0 or np.isnan(cutoff):\n",
    "        continue  # skip invalid cutoffs\n",
    "    thegeethika,queengeethika,not ur geethika,geethz,lil geet\n",
    "    sample_ranks = [\n",
    "        int(max(1, cutoff*0.5 + np.random.randint(-50,50))),\n",
    "        int(max(1, cutoff*0.8 + np.random.randint(-30,30))),\n",
    "        int(max(1, cutoff + np.random.randint(-20,20))),\n",
    "        int(max(1, cutoff*1.1 + np.random.randint(-30,60))),\n",
    "        int(max(1, cutoff*1.4 + np.random.randint(-50,100)))\n",
    "    ]\n",
    "    \n",
    "    for r in sample_ranks:\n",
    "        admit = 1 if r <= cutoff else 0\n",
    "        if np.random.rand() < 0.03:\n",
    "            admit = 1 - admit\n",
    "        prob = max(0.0, min(1.0, 1 - (r / (cutoff+1e-6))))  # add epsilon to avoid div/0\n",
    "        student_rows.append({\n",
    "            'exam': row['exam'],\n",
    "            'college': row['college'],\n",
    "            'branch': row['branch'],\n",
    "            'category': str(row['category']),\n",
    "            'rank': r,\n",
    "            'admit': admit,\n",
    "            'admit_prob': prob\n",
    "        })\n",
    "\n",
    "students = pd.DataFrame(student_rows)\n",
    "print('Synthetic students shape:', students.shape)\n",
    "students.head()\n",
    "\n",
    "students.sample(5).to_csv(os.path.join(OUTPUT_DIR,'students_sample.csv'), index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Remaining cells (encoding, model training, evaluation, etc.)\n",
    "# -----------------------------------------------------------------------------\n",
    "# [The rest of the notebook remains unchanged from the previous update]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 6: Feature encoding and train/test split\n",
    "# -----------------------------------------------------------------------------\n",
    "le_exam = LabelEncoder(); students['exam_enc']=le_exam.fit_transform(students['exam'])\n",
    "le_cat = LabelEncoder(); students['cat_enc']=le_cat.fit_transform(students['category'])\n",
    "le_branch = LabelEncoder(); students['branch_enc']=le_branch.fit_transform(students['branch'].astype(str))\n",
    "\n",
    "X = students[['rank','exam_enc','cat_enc','branch_enc']]\n",
    "y_clf = students['admit']\n",
    "y_reg = students['admit_prob']\n",
    "\n",
    "X_train, X_test, y_train_clf, y_test_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42, stratify=y_clf)\n",
    "X_train_r, X_test_r, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler(); X_train_scaled = scaler.fit_transform(X_train); X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 7: Define models to test\n",
    "# -----------------------------------------------------------------------------\n",
    "models_clf = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'SVM': SVC(probability=True, gamma='scale'),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(64,32), max_iter=500, random_state=42)\n",
    "}\n",
    "if 'XGBClassifier' in globals() and XGBClassifier is not None:\n",
    "    models_clf['XGBoost'] = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Regression models for probability estimation\n",
    "models_reg = {\n",
    "    'Huber': HuberRegressor(),\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 8: Safe Cross-validation & Training for Large Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "# Reduce dataset size for cross-validation to manage memory\n",
    "cv_sample_size = min(100000, len(X))  # max 100k rows\n",
    "X_cv_sample, _, y_cv_sample, _ = train_test_split(X, y_clf, train_size=cv_sample_size, random_state=42, stratify=y_clf)\n",
    "\n",
    "# Scale features for all models if needed\n",
    "scaler = StandardScaler()\n",
    "X_cv_scaled = scaler.fit_transform(X_cv_sample)\n",
    "\n",
    "# Models\n",
    "models_clf = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=2),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(64,32), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "if 'XGBClassifier' in globals() and XGBClassifier is not None:\n",
    "    models_clf['XGBoost'] = XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n",
    "                                           n_estimators=200, max_depth=6, tree_method='hist',\n",
    "                                           random_state=42, n_jobs=2)\n",
    "\n",
    "# Train and evaluate classification models\n",
    "for name, model in models_clf.items():\n",
    "    print(f'\\nTraining and cross-validating: {name}')\n",
    "    \n",
    "    # Cross-validation on sample\n",
    "    try:\n",
    "        acc_scores = cross_val_score(model, X_cv_sample, y_cv_sample, cv=cv, scoring='accuracy', n_jobs=2)\n",
    "        try:\n",
    "            roc_scores = cross_val_score(model, X_cv_sample, y_cv_sample, cv=cv, scoring='roc_auc', n_jobs=2)\n",
    "        except Exception:\n",
    "            roc_scores = np.array([np.nan]*len(acc_scores))\n",
    "        print(f'CV Accuracy mean: {acc_scores.mean():.4f} ± {acc_scores.std():.4f}')\n",
    "        print(f'CV ROC-AUC mean: {np.nanmean(roc_scores):.4f} ± {np.nanstd(roc_scores):.4f}')\n",
    "    except Exception as e:\n",
    "        print('CV failed:', e)\n",
    "        acc_scores, roc_scores = [np.nan],[np.nan]\n",
    "    \n",
    "    # Fit on full training set carefully\n",
    "    try:\n",
    "        model.fit(X_train, y_train_clf)\n",
    "        preds = model.predict(X_test)\n",
    "        probs = model.predict_proba(X_test)[:,1] if hasattr(model,'predict_proba') else model.decision_function(X_test)\n",
    "        acc = accuracy_score(y_test_clf, preds)\n",
    "        prec = precision_score(y_test_clf, preds)\n",
    "        rec = recall_score(y_test_clf, preds)\n",
    "        f1 = f1_score(y_test_clf, preds)\n",
    "        roc = roc_auc_score(y_test_clf, probs)\n",
    "        results[name] = {'cv_acc_mean':acc_scores.mean(),'cv_acc_std':acc_scores.std(),\n",
    "                         'cv_roc_mean':np.nanmean(roc_scores),'test_acc':acc,'test_prec':prec,\n",
    "                         'test_rec':rec,'test_f1':f1,'test_roc':roc}\n",
    "        print(f'Test Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}, ROC: {roc:.4f}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {name} on full dataset:\", e)\n",
    "        results[name] = {'cv_acc_mean':acc_scores.mean(),'cv_acc_std':acc_scores.std(),\n",
    "                         'cv_roc_mean':np.nanmean(roc_scores),'test_acc':np.nan,'test_prec':np.nan,\n",
    "                         'test_rec':np.nan,'test_f1':np.nan,'test_roc':np.nan}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Regression model (Huber) training safely\n",
    "# -----------------------------------------------------------------------------\n",
    "print('\\nTraining regression models (probability estimation):')\n",
    "X_cv_sample_r, _, y_cv_sample_reg, _ = train_test_split(X, y_reg, train_size=cv_sample_size, random_state=42)\n",
    "scaler_cv = StandardScaler(); X_cv_sample_scaled = scaler_cv.fit_transform(X_cv_sample_r)\n",
    "\n",
    "models_reg = {'Huber': HuberRegressor()}\n",
    "\n",
    "for name, model in models_reg.items():\n",
    "    print('Model:', name)\n",
    "    try:\n",
    "        cv_mse = cross_val_score(model, X_cv_sample_scaled, y_cv_sample_reg, cv=5,\n",
    "                                 scoring='neg_mean_squared_error', n_jobs=2)\n",
    "        print('CV MSE mean (sample):', -cv_mse.mean())\n",
    "    except Exception as e:\n",
    "        print('CV failed:', e)\n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train_reg)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test_reg, preds)\n",
    "        r2 = r2_score(y_test_reg, preds)\n",
    "        results[name] = {'test_mse':mse,'test_r2':r2}\n",
    "        print(f'Test MSE: {mse:.6f}, R2: {r2:.4f}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {name} on full dataset:\", e)\n",
    "        results[name] = {'test_mse':np.nan,'test_r2':np.nan}\n",
    "\n",
    "# Save results\n",
    "res_df = pd.DataFrame(results).T\n",
    "res_df.to_csv(os.path.join(OUTPUT_DIR,'model_comparison_results.csv'))\n",
    "print('Model comparison saved to outputs/model_comparison_results.csv')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 9: Best model selection & save (safe version)\n",
    "# -----------------------------------------------------------------------------\n",
    "best_model_name = None\n",
    "best_roc = -1\n",
    "\n",
    "# Classification models first\n",
    "for n, r in results.items():\n",
    "    if n in models_clf:\n",
    "        roc = r.get('test_roc', -1)\n",
    "        if roc is None: continue\n",
    "        try:\n",
    "            if roc > best_roc:\n",
    "                best_roc = roc\n",
    "                best_model_name = n\n",
    "        except Exception: continue\n",
    "\n",
    "# Regression models if no classifier performed well\n",
    "if best_model_name is None or best_roc <= 0:\n",
    "    best_r2 = -1\n",
    "    for n, r in results.items():\n",
    "        if n in models_reg:\n",
    "            r2 = r.get('test_r2', -1)\n",
    "            if r2 is None: continue\n",
    "            try:\n",
    "                if r2 > best_r2:\n",
    "                    best_r2 = r2\n",
    "                    best_model_name = n\n",
    "                    best_roc = r2\n",
    "            except Exception: continue\n",
    "\n",
    "print('\\nBest model by test ROC/R2:', best_model_name, 'score:', best_roc)\n",
    "\n",
    "# Persist best model\n",
    "import joblib\n",
    "if best_model_name in models_clf and not np.isnan(results.get(best_model_name, {}).get('test_acc', np.nan)):\n",
    "    joblib.dump(models_clf[best_model_name], os.path.join(OUTPUT_DIR, f'model_best_{best_model_name}.joblib'))\n",
    "elif best_model_name in models_reg and not np.isnan(results.get(best_model_name, {}).get('test_mse', np.nan)):\n",
    "    joblib.dump(models_reg[best_model_name], os.path.join(OUTPUT_DIR, f'model_best_{best_model_name}.joblib'))\n",
    "else:\n",
    "    print(f\"Could not save best model '{best_model_name}' as it either didn't perform well or fitting failed.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 9: Model usage helper\n",
    "# -----------------------------------------------------------------------------\n",
    "# Function to load best model and predict admission + probability\n",
    "\n",
    "def load_best_and_predict(rank, exam, category, branch):\n",
    "    # load encoders\n",
    "    ex_enc = le_exam.transform([exam])[0] if exam in le_exam.classes_ else 0\n",
    "    cat_enc = le_cat.transform([category])[0] if category in le_cat.classes_ else 0\n",
    "    br_enc = le_branch.transform([branch])[0] if branch in le_branch.classes_ else 0\n",
    "    sample = pd.DataFrame([[rank, ex_enc, cat_enc, br_enc]], columns=['rank','exam_enc','cat_enc','branch_enc'])\n",
    "    # load best model file\n",
    "    if best_model_name is None:\n",
    "        print('No best model determined.')\n",
    "        return None\n",
    "    path = os.path.join(OUTPUT_DIR, f'model_best_{best_model_name}.joblib')\n",
    "    if not os.path.exists(path):\n",
    "        print('Best model file not found:', path)\n",
    "        return None\n",
    "    model = joblib.load(path)\n",
    "    # if classifier\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred = model.predict(sample)[0]\n",
    "        prob = float(model.predict_proba(sample)[:,1][0])\n",
    "        return {'admit':bool(pred),'probability':prob,'model':best_model_name}\n",
    "    else:\n",
    "        # assume regression\n",
    "        sample_scaled = scaler.transform(sample)\n",
    "        prob = float(model.predict(sample_scaled)[0])\n",
    "        return {'admit': bool(prob>=0.5),'probability':prob,'model':best_model_name}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 10: Next steps & recommendations\n",
    "# -----------------------------------------------------------------------------\n",
    "print('\\nNext steps:')\n",
    "print('- Inspect outputs/model_comparison_results.csv to see model metrics side-by-side.')\n",
    "print('- If XGBoost was installed, it is included in the comparison and often performs best on tabular data.')\n",
    "print('- IMPORTANT: current training data is synthetic (sampled around cutoffs) → get real student admission outcomes to validate models properly.')\n",
    "print('- After selecting best model, integrate into your web app (Streamlit/Flask) with encoders saved (joblib).')\n",
    "print('- Optionally run feature importance (for tree models) to discover which features matter most.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4ca37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "Training Stacking Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Metrics:\n",
      "Accuracy: 0.9672, Precision: 0.9662, Recall: 0.9689, F1: 0.9675, ROC-AUC: 0.9698, MSE: 0.0307, R²: 0.8774\n",
      "\n",
      "Stacking Ensemble Metrics:\n",
      "Accuracy: 0.9698, Precision: 0.9700, Recall: 0.9703, F1: 0.9701, ROC-AUC: 0.9698, MSE: 0.0293, R²: 0.8827\n",
      "\n",
      "Best model saved: StackingClassifier(cv=5,\n",
      "                   estimators=[('xgb',\n",
      "                                XGBClassifier(base_score=None, booster=None,\n",
      "                                              callbacks=None,\n",
      "                                              colsample_bylevel=None,\n",
      "                                              colsample_bynode=None,\n",
      "                                              colsample_bytree=0.8, device=None,\n",
      "                                              early_stopping_rounds=None,\n",
      "                                              enable_categorical=False,\n",
      "                                              eval_metric='logloss',\n",
      "                                              feature_types=None,\n",
      "                                              feature_weights=None, gamma=None,\n",
      "                                              grow_policy=None,\n",
      "                                              importance_type=None,\n",
      "                                              interact...\n",
      "                                              max_delta_step=None, max_depth=6,\n",
      "                                              max_leaves=None,\n",
      "                                              min_child_weight=None,\n",
      "                                              missing=nan,\n",
      "                                              monotone_constraints=None,\n",
      "                                              multi_strategy=None,\n",
      "                                              n_estimators=300, n_jobs=None,\n",
      "                                              num_parallel_tree=None, ...)),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(max_depth=10,\n",
      "                                                       n_estimators=200,\n",
      "                                                       random_state=42)),\n",
      "                               ('log', LogisticRegression(max_iter=1000))],\n",
      "                   final_estimator=LogisticRegression(), n_jobs=2,\n",
      "                   passthrough=True)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Admission Prediction Notebook: XGBoost + Ensemble + Rank_ratio\n",
    "# =============================================================================\n",
    "\n",
    "# --- Cell 1: Imports ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# --- Cell 2: File paths (update with your real dataset paths) ---\n",
    "AP_FILE = \"/home/me3-lab/Project/data/AP_EAMCET_Cleaned_Merged.csv\"\n",
    "TS_FILE = \"/home/me3-lab/Project/data/TSEAMCET_2021_2022_2023_merged_clean.csv\"\n",
    "JEE_FILE = \"/home/me3-lab/Project/data/JEE_data.csv\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Cell 3: Load Data ---\n",
    "def load_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "ap_df = load_csv(AP_FILE)\n",
    "ts_df = load_csv(TS_FILE)\n",
    "jee_df = load_csv(JEE_FILE)\n",
    "\n",
    "# --- Cell 4: Build cutoff table ---\n",
    "records = []\n",
    "\n",
    "if ap_df is not None:\n",
    "    cat_cols = [c for c in ap_df.columns if any(k in c.upper() for k in ['BOYS','GIRLS','OC','BC','SC','ST','EWS'])]\n",
    "    for _, row in ap_df.iterrows():\n",
    "        for c in cat_cols:\n",
    "            try: cutoff = float(row[c])\n",
    "            except: continue\n",
    "            records.append({\n",
    "                'exam':'AP','college':row.get('NAME_OF_THE_INSTITUTION',None),\n",
    "                'branch':row.get('branch_code',row.get('branch',None)),\n",
    "                'category':c,'cutoff':cutoff\n",
    "            })\n",
    "\n",
    "if ts_df is not None:\n",
    "    cat_cols = [c for c in ts_df.columns if any(k in c.lower() for k in ['boys','girls','oc','bc','sc','st','ews'])]\n",
    "    for _, row in ts_df.iterrows():\n",
    "        for c in cat_cols:\n",
    "            try: cutoff = float(row[c])\n",
    "            except: continue\n",
    "            records.append({\n",
    "                'exam':'TS','college':row.get('institute_name',None),\n",
    "                'branch':row.get('branch',None),'category':c,'cutoff':cutoff\n",
    "            })\n",
    "\n",
    "if jee_df is not None and 'closing_rank' in jee_df.columns:\n",
    "    for _, row in jee_df.iterrows():\n",
    "        try: cutoff = float(row['closing_rank'])\n",
    "        except: continue\n",
    "        records.append({\n",
    "            'exam':'JEE','college':row.get('institute_short',None),\n",
    "            'branch':row.get('program_name',None),\n",
    "            'category':row.get('category',None),'cutoff':cutoff\n",
    "        })\n",
    "\n",
    "cutoffs_df = pd.DataFrame(records)\n",
    "\n",
    "# --- Cell 5: Synthetic student dataset ---\n",
    "np.random.seed(42)\n",
    "student_rows = []\n",
    "for _, row in cutoffs_df.iterrows():\n",
    "    cutoff = row['cutoff']\n",
    "    if cutoff <= 0 or np.isnan(cutoff): continue\n",
    "    sample_ranks = [\n",
    "        int(max(1, cutoff*0.5 + np.random.randint(-50,50))),\n",
    "        int(max(1, cutoff*0.8 + np.random.randint(-30,30))),\n",
    "        int(max(1, cutoff + np.random.randint(-20,20))),\n",
    "        int(max(1, cutoff*1.1 + np.random.randint(-30,60))),\n",
    "        int(max(1, cutoff*1.4 + np.random.randint(-50,100)))\n",
    "    ]\n",
    "    for r in sample_ranks:\n",
    "        admit = 1 if r <= cutoff else 0\n",
    "        if np.random.rand() < 0.03: admit = 1 - admit\n",
    "        student_rows.append({\n",
    "            'exam': row['exam'],'college': row['college'],'branch': row['branch'],\n",
    "            'category': str(row['category']),'rank': r,'admit': admit,\n",
    "            'rank_ratio': r/cutoff\n",
    "        })\n",
    "\n",
    "students = pd.DataFrame(student_rows)\n",
    "\n",
    "# --- Cell 6: Encode features ---\n",
    "le_exam = LabelEncoder(); students['exam_enc']=le_exam.fit_transform(students['exam'])\n",
    "le_cat = LabelEncoder(); students['cat_enc']=le_cat.fit_transform(students['category'])\n",
    "le_branch = LabelEncoder(); students['branch_enc']=le_branch.fit_transform(students['branch'].astype(str))\n",
    "\n",
    "X = students[['rank_ratio','exam_enc','cat_enc','branch_enc']]\n",
    "y = students['admit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Cell 7: Scale numeric features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Cell 8: Define models ---\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    eval_metric='logloss', random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model), ('log', log_model)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5, n_jobs=2, passthrough=True\n",
    ")\n",
    "\n",
    "# --- Cell 9: Train models ---\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Stacking Ensemble...\")\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Cell 10: Evaluation ---\n",
    "def evaluate_model(model, X_test, y_test, name=\"Model\"):\n",
    "    preds = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)[:,1]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds)\n",
    "    rec = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    roc = roc_auc_score(y_test, probs)\n",
    "    mse = mean_squared_error(y_test, probs)\n",
    "    r2 = r2_score(y_test, probs)\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, \"\n",
    "          f\"F1: {f1:.4f}, ROC-AUC: {roc:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec,\n",
    "            'f1': f1, 'roc_auc': roc, 'mse': mse, 'r2': r2}\n",
    "\n",
    "xgb_metrics = evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "stack_metrics = evaluate_model(stack_model, X_test, y_test, \"Stacking Ensemble\")\n",
    "\n",
    "# --- Cell 11: Save best model ---\n",
    "best_model = xgb_model if xgb_metrics['roc_auc'] >= stack_metrics['roc_auc'] else stack_model\n",
    "joblib.dump(best_model, os.path.join(OUTPUT_DIR,'best_admission_model.joblib'))\n",
    "print(\"\\nBest model saved:\", best_model)\n",
    "\n",
    "# --- Cell 12: Prediction function ---\n",
    "def predict_admission(rank, exam, category, branch):\n",
    "    try: exam_enc = le_exam.transform([exam])[0]\n",
    "    except: exam_enc = 0\n",
    "    try: cat_enc = le_cat.transform([category])[0]\n",
    "    except: cat_enc = 0\n",
    "    try: branch_enc = le_branch.transform([branch])[0]\n",
    "    except: branch_enc = 0\n",
    "\n",
    "    subset = cutoffs_df[(cutoffs_df.exam==exam)&(cutoffs_df.branch==branch)&(cutoffs_df.category==category)]\n",
    "    cutoff = subset['cutoff'].values[0] if len(subset)>0 else rank\n",
    "    rank_ratio = rank / cutoff\n",
    "\n",
    "    sample = pd.DataFrame([[rank_ratio, exam_enc, cat_enc, branch_enc]],\n",
    "                          columns=['rank_ratio','exam_enc','cat_enc','branch_enc'])\n",
    "    prob = best_model.predict_proba(sample)[:,1][0]\n",
    "    admit = 1 if prob >= 0.5 else 0\n",
    "    return {'admit': bool(admit), 'probability': prob}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e747a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders and scaler saved to outputs/\n"
     ]
    }
   ],
   "source": [
    "import joblib, os\n",
    "\n",
    "# Save encoders & scaler\n",
    "joblib.dump(le_exam, os.path.join(OUTPUT_DIR, \"le_exam.joblib\"))\n",
    "joblib.dump(le_cat, os.path.join(OUTPUT_DIR, \"le_cat.joblib\"))\n",
    "joblib.dump(le_branch, os.path.join(OUTPUT_DIR, \"le_branch.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(OUTPUT_DIR, \"scaler.joblib\"))\n",
    "\n",
    "print(\"Encoders and scaler saved to outputs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0642c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/le_college.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Create LabelEncoder for college\n",
    "le_college = LabelEncoder()\n",
    "students['college_enc'] = le_college.fit_transform(students['college'].astype(str))\n",
    "\n",
    "# Update X to include college_enc\n",
    "X = students[['rank_ratio','exam_enc','cat_enc','branch_enc','college_enc']]\n",
    "\n",
    "# Save the encoder\n",
    "joblib.dump(le_college, 'outputs/le_college.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316f6655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['outputs/best_admission_model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "y = students['admit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Optionally scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train stacking ensemble\n",
    "xgb_model = XGBClassifier(n_estimators=300, max_depth=6, eval_metric='logloss', random_state=42)\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model), ('log', log_model)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5, n_jobs=2, passthrough=True\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(stack_model, 'outputs/best_admission_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f6afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)  # make sure folder exists\n",
    "cutoffs_df.to_csv('outputs/cutoffs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae33f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/me3-lab/Project/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Admission Prediction Notebook: Corrected Version\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils import resample\n",
    "import joblib\n",
    "\n",
    "# --- Directories ---\n",
    "DATA_DIR = \"/home/me3-lab/Project/data\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load datasets ---\n",
    "ap_df = pd.read_csv(os.path.join(DATA_DIR, \"/home/me3-lab/Project/data/AP_EAMCET_Cleaned_Merged.csv\"))\n",
    "ts_df = pd.read_csv(os.path.join(DATA_DIR, \"/home/me3-lab/Project/data/TSEAMCET_2021_2022_2023_merged_clean.csv\"))\n",
    "jee_df = pd.read_csv(os.path.join(DATA_DIR, \"/home/me3-lab/Project/data/JEE_data.csv\"))\n",
    "\n",
    "# --- Build cutoff table ---\n",
    "records = []\n",
    "\n",
    "def add_cutoffs(df, exam_col_name, college_col_name, branch_col_name, cat_cols, exam_name):\n",
    "    for _, row in df.iterrows():\n",
    "        for c in cat_cols:\n",
    "            try:\n",
    "                cutoff = float(row[c])\n",
    "            except:\n",
    "                continue\n",
    "            records.append({\n",
    "                'exam': exam_name,\n",
    "                'college': row.get(college_col_name),\n",
    "                'branch': row.get(branch_col_name),\n",
    "                'category': c,\n",
    "                'cutoff': cutoff\n",
    "            })\n",
    "\n",
    "# AP\n",
    "cat_cols = [c for c in ap_df.columns if any(k in c.upper() for k in ['BOYS','GIRLS','OC','BC','SC','ST','EWS'])]\n",
    "add_cutoffs(ap_df, 'exam','NAME_OF_THE_INSTITUTION','branch',cat_cols,'AP')\n",
    "\n",
    "# TS\n",
    "cat_cols = [c for c in ts_df.columns if any(k in c.lower() for k in ['boys','girls','oc','bc','sc','st','ews'])]\n",
    "add_cutoffs(ts_df, 'exam','institute_name','branch',cat_cols,'TS')\n",
    "\n",
    "# JEE\n",
    "if 'closing_rank' in jee_df.columns:\n",
    "    for _, row in jee_df.iterrows():\n",
    "        try: cutoff = float(row['closing_rank'])\n",
    "        except: continue\n",
    "        records.append({\n",
    "            'exam':'JEE',\n",
    "            'college':row.get('institute_short', None),\n",
    "            'branch':row.get('program_name', None),\n",
    "            'category':row.get('category', None),\n",
    "            'cutoff': cutoff\n",
    "        })\n",
    "\n",
    "cutoffs_df = pd.DataFrame(records)\n",
    "cutoffs_df.to_csv(os.path.join(OUTPUT_DIR,'cutoffs.csv'), index=False)\n",
    "\n",
    "# --- Generate synthetic student data for training ---\n",
    "student_rows = []\n",
    "np.random.seed(42)\n",
    "for _, row in cutoffs_df.iterrows():\n",
    "    cutoff = row['cutoff']\n",
    "    if np.isnan(cutoff) or cutoff <= 0: continue\n",
    "    # generate multiple ranks around cutoff\n",
    "    sample_ranks = [\n",
    "        int(max(1, cutoff*0.5 + np.random.randint(-50,50))),\n",
    "        int(max(1, cutoff*0.8 + np.random.randint(-30,30))),\n",
    "        int(max(1, cutoff + np.random.randint(-20,20))),\n",
    "        int(max(1, cutoff*1.1 + np.random.randint(-30,60))),\n",
    "        int(max(1, cutoff*1.4 + np.random.randint(-50,100)))\n",
    "    ]\n",
    "    for r in sample_ranks:\n",
    "        admit = 1 if r <= cutoff else 0\n",
    "        if np.random.rand() < 0.03: admit = 1 - admit\n",
    "        student_rows.append({\n",
    "            'exam': row['exam'],\n",
    "            'college': row['college'],\n",
    "            'branch': row['branch'],\n",
    "            'category': str(row['category']),\n",
    "            'rank': r,\n",
    "            'admit': admit,\n",
    "            'rank_ratio': r/cutoff\n",
    "        })\n",
    "\n",
    "students = pd.DataFrame(student_rows)\n",
    "\n",
    "# --- Encode categorical features ---\n",
    "le_exam = LabelEncoder(); students['exam_enc'] = le_exam.fit_transform(students['exam'].astype(str))\n",
    "le_cat = LabelEncoder(); students['cat_enc'] = le_cat.fit_transform(students['category'].astype(str))\n",
    "le_branch = LabelEncoder(); students['branch_enc'] = le_branch.fit_transform(students['branch'].astype(str))\n",
    "le_college = LabelEncoder(); students['college_enc'] = le_college.fit_transform(students['college'].astype(str))\n",
    "\n",
    "X = students[['rank_ratio','exam_enc','cat_enc','branch_enc','college_enc']]\n",
    "y = students['admit']\n",
    "\n",
    "# --- Balance classes ---\n",
    "df1 = students[students.admit==1]\n",
    "df0 = students[students.admit==0]\n",
    "df0_up = resample(df0, replace=True, n_samples=len(df1), random_state=42)\n",
    "students_balanced = pd.concat([df1, df0_up])\n",
    "X = students_balanced[['rank_ratio','exam_enc','cat_enc','branch_enc','college_enc']]\n",
    "y = students_balanced['admit']\n",
    "\n",
    "# --- Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Scale rank_ratio only (optional) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled['rank_ratio'] = scaler.fit_transform(X_train[['rank_ratio']])\n",
    "X_test_scaled['rank_ratio'] = scaler.transform(X_test[['rank_ratio']])\n",
    "\n",
    "# --- Define models ---\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    eval_metric='logloss', random_state=42\n",
    ")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model), ('log', log_model)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5, passthrough=True\n",
    ")\n",
    "\n",
    "# --- Calibrate probabilities ---\n",
    "stack_model = CalibratedClassifierCV(stack_model, cv=5)\n",
    "stack_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)[:,1]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    roc = roc_auc_score(y_test, probs)\n",
    "    print(f\"Accuracy: {acc:.4f}, ROC-AUC: {roc:.4f}\")\n",
    "\n",
    "evaluate_model(stack_model, X_test_scaled, y_test)\n",
    "\n",
    "# --- Save model and encoders ---\n",
    "joblib.dump(stack_model, os.path.join(OUTPUT_DIR,'best_admission_model.joblib'))\n",
    "joblib.dump(le_exam, os.path.join(OUTPUT_DIR,'le_exam.joblib'))\n",
    "joblib.dump(le_cat, os.path.join(OUTPUT_DIR,'le_cat.joblib'))\n",
    "joblib.dump(le_branch, os.path.join(OUTPUT_DIR,'le_branch.joblib'))\n",
    "joblib.dump(le_college, os.path.join(OUTPUT_DIR,'le_college.joblib'))\n",
    "\n",
    "print(\"Training complete. All outputs saved in 'outputs/' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
